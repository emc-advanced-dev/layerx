# -*- mode: ruby -*-
# vi: set ft=ruby :
require 'yaml'
require './lib/gen_node_infos'
require './lib/predicates'
require './lib/helpers'
require './lib/deploy_kubernetes'

base_dir = File.expand_path(File.dirname(__FILE__))
conf = YAML.load_file(File.join(base_dir, "cluster.yml"))
ninfos = gen_node_infos(conf)

# Kubernetes
if conf["deploy_kubernetes"]
  handle_kubernetes_action conf
end

## vagrant plugins required:
# vagrant-aws, vagrant-berkshelf, vagrant-omnibus, vagrant-hosts, vagrant-cachier
Vagrant.configure("2") do |config|
  if !conf["custom_ami"] then
    # https://vagrantcloud.com/everpeace/boxes/mesos
    config.vm.box = "everpeace/mesos"
  end

  # enable plugins
  config.berkshelf.enabled = true
  config.berkshelf.berksfile_path ="./Berksfile"
  config.omnibus.chef_version = '12.10.24'

  # if you want to use vagrant-cachier,
  # please install vagrant-cachier plugin.
  if Vagrant.has_plugin?("vagrant-cachier")
    config.cache.enable :apt
    config.cache.enable :chef
  end

  is_plugin("vagrant-berkshelf")
  is_plugin("vagrant-omnibus")
  is_plugin("vagrant-hosts")

  # define VMs. all VMs have identical configuration.
  [ninfos[:zk], ninfos[:master], ninfos[:slave]].flatten.each_with_index do |ninfo, i|
    config.vm.define ninfo[:hostname] do |cfg|

      cfg.vm.provider :virtualbox do |vb, override|
        override.vm.hostname = ninfo[:hostname]
        override.vm.network :private_network, :ip => ninfo[:ip]
        override.vm.provision :hosts

        vb.name = 'vagrant-mesos-' + ninfo[:hostname]
        vb.customize ["modifyvm", :id, "--memory", ninfo[:mem], "--cpus", ninfo[:cpus] ]

        override.vm.provision :shell do |s|
          s.path = "scripts/populate_sshkey.sh"
          s.args = "/root root"
        end

        override.vm.provision :shell do |s|
          s.path = "scripts/populate_sshkey.sh"
          s.args = "/home/vagrant vagrant"
        end
      end

      # mesos-master doesn't create its work_dir.
      master_work_dir = "/var/run/mesos"
      if master?(ninfo[:hostname]) then
        cfg.vm.provision :shell, :inline => "mkdir -p #{master_work_dir}"
      end


      if master?(ninfo[:hostname]) then
        #install marathon on masters
        cfg.vm.provision "shell", path: "install-marathon.sh"
      end


      cfg.vm.provision :chef_solo do |chef|
        chef.log_level = :debug
        chef.channel = "stable"
        chef.version = "12.10.24"
        chef.add_recipe "apt"

        if master?(ninfo[:hostname]) then

          #upload kubeconfig
          kubeconfig_home = cfg["kube_cfg_home"]

          if conf["deploy_kubernetes"]
            cfg.vm.provision "shell", inline: "mkdir -p #{kubeconfig_home}"
            cfg.vm.provision "file", source: "./minikube/kubeconfig", destination: kubeconfig_home
            cfg.vm.provision "file", source: "./minikube/ca.crt", destination: kubeconfig_home
            cfg.vm.provision "file", source: "./minikube/apiserver.crt", destination: kubeconfig_home
            cfg.vm.provision "file", source: "./minikube/apiserver.key", destination: kubeconfig_home
          end

          chef.add_recipe "mesos::master"
          chef.add_recipe "golang"
          chef.add_recipe "golang::packages"
          chef.add_recipe "layerx"
          chef.json  = {
              :go => {
                :version => "1.7",
                :owner => "vagrant",
                :group => "vagrant",
                :packages => [
                  "github.com/jteeuwen/go-bindata/...",
                  "github.com/elazarl/go-bindata-assetfs/..."
                ]
              },
              :layerx => {
                :user => "vagrant",
                :group => "vagrant",
                :bind_address => ninfos[:master].map{|master| master[:ip]}[0],
                :deploy_marathon => true,
                :deploy_kubernetes => conf["deploy_kubernetes"],
                :kubeconfig => "#{kubeconfig_home}/kubeconfig",
              },
              :mesos=> {
                :mesosphere => {
                  build_version: conf["mesos_build_version"]
                },
                :type         => "mesosphere",
                :version      => conf["mesos_version"],
                :master_ips   => ninfos[:master].map { |m| "#{m[:ip]}" },
                :slave_ips    => ninfos[:slave].map { |s| "#{s[:ip]}" },
                :master       => if ninfos[:zk].length > 0 then
                  {
                    :cluster => "MyCluster",
                    :quorum => "#{(ninfos[:master].length.to_f/2).ceil}",
                    :work_dir => master_work_dir,
                    :zk => "zk://"+ninfos[:zk].map{|zk| zk[:ip]+":2181"}.join(", ")+"/mesos",
                    :ip => "#{ninfo[:ip]}"
                  }
                else
                  {
                    :cluster => "MyCluster",
                    :quorum => "#{(ninfos[:master].length.to_f/2).ceil}",
                    :work_dir => master_work_dir,
                    :ip => "#{ninfo[:ip]}"
                  }
                end
            }
          }

        elsif slave?(ninfo[:hostname]) then
          chef.add_recipe "mesos::slave"

          chef.json = {
            :mesos => {
                :mesosphere => {
                    build_version: conf["mesos_build_version"]
                },
              :type         => "mesosphere",
              :version      => conf["mesos_version"],
              :slave        => {
                :master       => if ninfos[:zk].length > 0 then
                                   "zk://"+ninfos[:zk].map{|zk| zk[:ip]+":2181"}.join(", ")+"/mesos"
                                 else
                                   "#{ninfos[:master][0][:ip]}:5050"
                                 end,
                :ip           => "#{ninfo[:ip]}",
                :containerizers => "docker,mesos",
                :isolation => "cgroups/cpu,cgroups/mem",
                :hostname => "#{ninfo[:ip]}",
                :executor_registration_timeout => "5mins",
                :resources => "ports:[31000-32000,4040-4040,5432-5433,6000-7001,7080,7081,7199-7199,8012,8983,9042-9042,9160-9160,27000-29000,61621-61621]",
              }
            }
          }

        end
      end

      cfg.vm.provision :shell, :inline => <<-SCRIPT
      echo """172.31.1.11 master1
      172.31.1.12 master2
      172.31.2.11 slave1
      172.31.2.12 slave2
      172.31.2.13 slave3
      172.31.2.14 slave4""" | sudo tee -a /etc/hosts
      SCRIPT

      if zk?(ninfo[:hostname]) then
        myid = (/zk([0-9]+)/.match ninfo[:hostname])[1]
        cfg.vm.provision :shell, :inline => <<-SCRIPT
          sudo mkdir -p /tmp/zookeeper
          sudo chmod 755 /tmp/zookeeper
          sudo chown zookeeper /tmp/zookeeper
          sudo -u zookeeper echo #{myid} > /tmp/zookeeper/myid
          sudo -u zookeeper /opt/chef/embedded/bin/ruby /vagrant/scripts/gen_zoo_conf.rb > /etc/zookeeper/conf/zoo.cfg
          sudo restart zookeeper
        SCRIPT
      end

      # If you wanted use `.dockercfg` file
      # Please place the file simply on this directory
      # if File.exist?(".dockercfg")
      #   config.vm.provision :shell, :priviledged => true, :inline => <<-SCRIPT
      #     cp /vagrant/.dockercfg /root/.dockercfg
      #     chmod 600 /root/.dockercfg
      #     chown root /root/.dockercfg
      #   SCRIPT
      # end
    end
  end


end
